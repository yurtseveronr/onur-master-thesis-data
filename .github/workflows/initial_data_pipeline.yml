name: Data Setup
on:
  workflow_dispatch:
permissions:
  id-token: write
  contents: read
jobs:
  deploy-and-upload-all:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository with LFS
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          lfs: true
          
      - name: Configure Git LFS and checkout files
        run: |
          git lfs install
          git lfs pull
          
      - name: Verify LFS files are downloaded
        run: |
          echo "Checking LFS files status:"
          git lfs ls-files -l
          echo ""
          echo "CSV file sizes:"
          find . -name "*.csv" -exec ls -lh {} \;
          
      - name: Configure AWS Credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::708778582346:role/onur-github-role
          aws-region: us-east-1
          
      - name: Deploy S3 CloudFormation Stack
        run: |
          STACK_NAME="s3-bucket-stack"
          REGION="us-east-1"
          TEMPLATE_FILE="cloudformation_templates/s3.yml"

          if aws cloudformation describe-stacks --stack-name $STACK_NAME --region $REGION > /dev/null 2>&1; then
            echo "Stack $STACK_NAME already exists, skipping deploy."
          else
            echo "Deploying stack $STACK_NAME..."
            aws cloudformation deploy \
              --template-file $TEMPLATE_FILE \
              --stack-name $STACK_NAME \
              --region $REGION \
              --capabilities CAPABILITY_NAMED_IAM
          fi

            
      - name: Retrieve Bucket Name
        id: get-bucket
        run: |
          # Try to get bucket name from CloudFormation
          BUCKET_NAME=$(aws cloudformation describe-stacks \
            --stack-name s3-bucket-stack \
            --region us-east-1 \
            --query "Stacks[0].Outputs[?OutputKey=='BucketName'].OutputValue" \
            --output text 2>/dev/null || echo "None")
          
          # If CloudFormation fails or returns None, use fallback
          if [ "$BUCKET_NAME" = "None" ] || [ -z "$BUCKET_NAME" ]; then
            echo "CloudFormation bucket name not found, using fallback: onur-master-dataset"
            BUCKET_NAME="onur-master-dataset"
          fi
          
          echo "Using bucket: $BUCKET_NAME"
          echo "bucket=$BUCKET_NAME" >> $GITHUB_OUTPUT
            
      - name: Upload all files to S3
        run: |
          BUCKET=${{ steps.get-bucket.outputs.bucket }}
          echo "Using bucket: $BUCKET"
          
          echo "Uploading all files to s3://$BUCKET/"
          
          # Find and upload all files (excluding git and system folders)
          find . -type f \
            ! -path "./.git/*" \
            ! -path "./.github/*" \
            ! -path "*/node_modules/*" \
            ! -path "*/\.DS_Store" \
            ! -path "*/__pycache__/*" \
            ! -path "*/\.vscode/*" \
            ! -path "*/\.idea/*" \
          | sed 's|^\./||' \
          | while read file; do
              if [ -f "$file" ]; then
                file_size=$(stat -c%s "$file" 2>/dev/null || stat -f%z "$file" 2>/dev/null || echo "unknown")
                file_ext="${file##*.}"
                
                echo "â†’ Uploading: $file (size: $file_size bytes, type: $file_ext)"
                
                if [[ "$file_ext" == "csv" ]] || [[ "$file_ext" == "CSV" ]]; then
                  echo "CSV file detected - using LFS optimized upload"
                  
                  aws s3 cp "$file" "s3://$BUCKET/$file" --storage-class STANDARD
                # Use multipart upload for other large files
                elif [ "$file_size" != "unknown" ] && [ "$file_size" -gt 104857600 ]; then
                  echo "  ðŸ“¦ Large file detected - using multipart upload"
                  aws s3 cp "$file" "s3://$BUCKET/$file" --storage-class STANDARD
                else
                  aws s3 cp "$file" "s3://$BUCKET/$file"
                fi
              else
                echo "File not found: $file"
              fi
            done
            
      - name: Verify upload success
        run: |
          BUCKET=${{ steps.get-bucket.outputs.bucket }}
          echo "Verifying uploaded files:"
          echo ""
          echo "CSV files:"
          aws s3 ls "s3://$BUCKET/" --recursive --human-readable | grep -i "\.csv" || echo "No CSV files found"
          echo ""
          echo "All files:"
          aws s3 ls "s3://$BUCKET/" --recursive --human-readable | head -20
          echo ""
          echo "Total file count:"
          aws s3 ls "s3://$BUCKET/" --recursive | wc -l

      - name: Glue and DynamoDB setup
        run: |
          STACK_NAME="data"
          REGION="us-east-1"
          TEMPLATE_FILE="cloudformation_templates/data.yml"

          if aws cloudformation describe-stacks --stack-name $STACK_NAME --region $REGION > /dev/null 2>&1; then
            echo "Stack $STACK_NAME already exists, skipping deploy."
          else
            echo "Deploying stack $STACK_NAME..."
            aws cloudformation deploy \
              --template-file $TEMPLATE_FILE \
              --stack-name $STACK_NAME \
              --region $REGION \
              --capabilities CAPABILITY_NAMED_IAM
          fi

      - name: Run Glue Jobs
        shell: bash
        run: |
          set -euo pipefail
          REGION="us-east-1"
          JOBS=("movies_etl_job" "tv-series_etl_job")

          declare -a RUN_IDS

          echo "Starting Glue jobs in parallel..."
          for i in "${!JOBS[@]}"; do
            RUN_IDS[$i]=$(aws glue start-job-run \
              --region "$REGION" \
              --job-name "${JOBS[$i]}" \
              --query 'JobRunId' \
              --output text)
            echo "  - ${JOBS[$i]} started (runId: ${RUN_IDS[$i]})"
          done

          echo "Waiting until both jobs finish..."
          while true; do
            STATUS0=$(aws glue get-job-run --region "$REGION" --job-name "${JOBS[0]}" --run-id "${RUN_IDS[0]}" --query 'JobRun.JobRunState' --output text)
            STATUS1=$(aws glue get-job-run --region "$REGION" --job-name "${JOBS[1]}" --run-id "${RUN_IDS[1]}" --query 'JobRun.JobRunState' --output text)
            echo "$(date -u +'%Y-%m-%dT%H:%M:%SZ') | ${JOBS[0]}: $STATUS0 | ${JOBS[1]}: $STATUS1"

            case "$STATUS0" in SUCCEEDED|FAILED|ERROR|TIMEOUT|STOPPED) DONE0=1;; *) DONE0=0;; esac
            case "$STATUS1" in SUCCEEDED|FAILED|ERROR|TIMEOUT|STOPPED) DONE1=1;; *) DONE1=0;; esac

            [[ $DONE0 -eq 1 && $DONE1 -eq 1 ]] && break
            sleep 15
          done

          if [[ "$STATUS0" == "SUCCEEDED" && "$STATUS1" == "SUCCEEDED" ]]; then
            echo "Both Glue jobs completed successfully."
            exit 0
          else
            echo "One or both jobs failed. (${JOBS[0]}=$STATUS0, ${JOBS[1]}=$STATUS1)"
            exit 1
          fi

      - name: Import initial data to DynamoDB
        run: |
          python3 scripts/dynamodb_initial_upload.py

      - name: Wait 2 seconds
        run: sleep 2

      - name: Upload Kinesis
        run: |
            STACK_NAME="kinesis"
            REGION="us-east-1"
            TEMPLATE_FILE="cloudformation_templates/kinesis.yml"

            if aws cloudformation describe-stacks --stack-name $STACK_NAME --region $REGION > /dev/null 2>&1; then
              echo "Stack $STACK_NAME already exists, skipping deploy."
            else
              echo "Deploying stack $STACK_NAME..."
              aws cloudformation deploy \
                --template-file $TEMPLATE_FILE \
                --stack-name $STACK_NAME \
                --region $REGION \
                --capabilities CAPABILITY_NAMED_IAM
            fi

      - name: Upload Kinesis data 
        run: |
            python3 scripts/kinesis_initial_upload.py


      - name: QuickSight Creation
        run: |
            STACK_NAME="quicksight"
            REGION="us-east-1"
            TEMPLATE_FILE="cloudformation_templates/quicksight.yml"

            if aws cloudformation describe-stacks --stack-name $STACK_NAME --region $REGION > /dev/null 2>&1; then
              echo "Stack $STACK_NAME already exists, skipping deploy."
            else
              echo "Deploying stack $STACK_NAME..."
              aws cloudformation deploy \
                --template-file $TEMPLATE_FILE \
                --stack-name $STACK_NAME \
                --region $REGION \
                --capabilities CAPABILITY_NAMED_IAM
            fi




       
          






